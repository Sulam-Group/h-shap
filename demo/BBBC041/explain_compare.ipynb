{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining the Predictions of a Convolutional Neural Network on Blood Smears with Different Methods\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sulam-Group/h-shap/blob/jaco/chembe_ml/demo/BBBC041/explain_compare.ipynb)\n",
    "\n",
    "## Task\n",
    "\n",
    "To explain the predictions of a convolutional neural network trained to predict the presence of trophozoites (o.e., malaria infected cells) in human blood smears. The network was trained on the [BBBC041 dataset](https://data.broadinstitute.org/bbbc/BBBC041/).\n",
    "\n",
    "## Requirements\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the device to run on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained model\n",
    "torch.set_grad_enabled(False)\n",
    "model = torch.hub.load(\n",
    "    \"Sulam-Group/h-shap:jaco/chembe_ml\", \"bbbc041trophozoitenet\", trust_repo=\"check\"\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Model to Predict on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests as req\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as t\n",
    "import torchvision.transforms.functional as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def download(url: str) -> io.BytesIO:\n",
    "    \"\"\"\n",
    "    A helper function to download an object from a url.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    url: str\n",
    "        The url to download the object from.\n",
    "    \"\"\"\n",
    "    res = req.get(url)\n",
    "    res.raise_for_status()\n",
    "    return io.BytesIO(res.content)\n",
    "\n",
    "\n",
    "def annotate(cell_df: pd.DataFrame, ax: plt.Axes) -> None:\n",
    "    \"\"\"\n",
    "    A helper function to annotate an image with its ground-truth\n",
    "    annotations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cell_Df: pd.DataFrame\n",
    "        The DataFrame containing the cell bounding boxes.\n",
    "    ax: plt.Axes\n",
    "        The Axes containing the image to annotate.\n",
    "    \"\"\"\n",
    "    for c in cell_df:\n",
    "        category = c[\"category\"]\n",
    "        if category == \"trophozoite\":\n",
    "            bbox = c[\"bounding_box\"]\n",
    "            ul_r = bbox[\"minimum\"][\"r\"]\n",
    "            ul_c = bbox[\"minimum\"][\"c\"]\n",
    "            br_r = bbox[\"maximum\"][\"r\"]\n",
    "            br_c = bbox[\"maximum\"][\"c\"]\n",
    "            w = abs(br_c - ul_c)\n",
    "            h = abs(br_r - ul_r)\n",
    "            bbox = patches.Rectangle(\n",
    "                (ul_c, ul_r), w, h, linewidth=2, edgecolor=\"g\", facecolor=\"none\"\n",
    "            )\n",
    "            ax.add_patch(bbox)\n",
    "\n",
    "\n",
    "# Download images and ground truth annotations\n",
    "base_url = (\n",
    "    \"https://raw.githubusercontent.com/Sulam-Group/h-shap/jaco/chembe_ml/demo/BBBC041\"\n",
    ")\n",
    "image_ids = [\n",
    "    \"1f8f08ea-b5b3-4f68-94d4-3cc071b7dce8\",\n",
    "    \"1fdf99d1-a494-4174-bcd7-efbe457ab899\",\n",
    "    \"2aeb7b85-7df9-4a63-8d37-2eecaaa190e7\",\n",
    "    \"2bed2796-75a9-4d93-bb2d-e3ccbe1a5cbe\",\n",
    "    \"2f6224be-50d0-4e85-94ef-88315df561b6\",\n",
    "]\n",
    "images = [\n",
    "    Image.open(download(f\"{base_url}/images/{image_id}.png\")) for image_id in image_ids\n",
    "]\n",
    "annotations = pd.read_json(download(f\"{base_url}/annotations.json\"))\n",
    "annotations.set_index(\"image\", inplace=True)\n",
    "\n",
    "# Define preprocessing transform\n",
    "transform = t.Compose(\n",
    "    [t.ToTensor(), t.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "# Visualize images, ground truth annotations, and predictions\n",
    "_, axes = plt.subplots(1, len(images), figsize=(16, 9))\n",
    "for i in range(len(images)):\n",
    "    # Preprocess image for prediction\n",
    "    x = images[i]\n",
    "    x = transform(x)\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "    # Use the model to precict the presence of trophozoites\n",
    "    x = x.to(device)\n",
    "    output = model(x)\n",
    "    prediction = torch.argmax(output, dim=1)\n",
    "\n",
    "    # Visualie image, ground truth annotations, and prediction\n",
    "    ax = axes[i]\n",
    "    ax.imshow(images[i])\n",
    "\n",
    "    cell_df = annotations.at[f\"{image_ids[i]}.png\", \"objects\"]\n",
    "    annotate(cell_df, ax)\n",
    "\n",
    "    ax.set_title(f\"Prediction: {prediction.item()}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install grad-cam --upgrade\n",
    "!python -m pip install lime --upgrade\n",
    "!python -m pip install shap --upgrade\n",
    "!python -m pip install h-shap --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reference Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference value used to mask features\n",
    "reference = torch.load(download(f\"{base_url}/reference.pt?raw=true\"))\n",
    "reference = reference.to(device)\n",
    "\n",
    "# Check that the prediction of the model on the reference is 0\n",
    "reference_output = model(reference.unsqueeze(0))\n",
    "reference_prediction = torch.argmax(reference_output, dim=1).cpu().item()\n",
    "\n",
    "# Visualize reference\n",
    "denorm_reference = tf.normalize(\n",
    "    reference.cpu(),\n",
    "    mean=[-0.485, -0.456, -0.406],\n",
    "    std=[1 / 0.229, 1 / 0.224, 1 / 0.225],\n",
    ")\n",
    "\n",
    "_, ax = plt.subplots(figsize=(16 / 2, 9 / 2))\n",
    "ax.imshow(denorm_reference.permute(1, 2, 0))\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(f\"Reference value (prediction = {reference_prediction})\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Explainers and Explanation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as f\n",
    "import shap\n",
    "import hshap\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from lime import lime_image\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "\n",
    "# Grad-CAM initialization function\n",
    "def _cam_init():\n",
    "    explainer = GradCAM(model=model, target_layers=[model.layer4[-1]], use_cuda=True)\n",
    "    return explainer\n",
    "\n",
    "\n",
    "# LIME initialization function\n",
    "def _lime_init():\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    return explainer\n",
    "\n",
    "\n",
    "# PartitionExplainer initialization function\n",
    "def _partexp_init():\n",
    "    partexp_reference = reference.permute(1, 2, 0)\n",
    "    partexp_reference = partexp_reference.cpu().numpy()\n",
    "    masker = shap.maskers.Image(partexp_reference)\n",
    "\n",
    "    def _f(x):\n",
    "        x = torch.tensor(x).float()\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = x.to(device)\n",
    "        output = model(x).cpu().numpy()[..., -1]\n",
    "        return output\n",
    "\n",
    "    explainer = shap.Explainer(_f, masker)\n",
    "    return explainer\n",
    "\n",
    "\n",
    "# h-Shap initialization function\n",
    "def _hshap_init():\n",
    "    explainer = hshap.Explainer(model=model, background=reference)\n",
    "    return explainer\n",
    "\n",
    "\n",
    "# Grad-CAM explanation function\n",
    "def _cam_explain(explainer, image):\n",
    "    image_t = transform(image)\n",
    "    image_t = image_t.to(device)\n",
    "    image_t = image_t.unsqueeze(0)\n",
    "\n",
    "    explanation = explainer(input_tensor=image_t).squeeze()\n",
    "    return explanation\n",
    "\n",
    "\n",
    "# LIME explanation function\n",
    "def _lime_explain(explainer, image):\n",
    "    def _f(x):\n",
    "        x = torch.stack(tuple(transform(u) for u in x), dim=0)\n",
    "        x = x.to(device)\n",
    "        output = model(x)\n",
    "        output = f.softmax(output, dim=1)\n",
    "        output = output.cpu().numpy()\n",
    "        return output\n",
    "\n",
    "    segmentation_fn = SegmentationAlgorithm(\n",
    "        \"quickshift\", kernel_size=4, max_dist=200, ratio=0.2\n",
    "    )\n",
    "\n",
    "    image_rgb = image.convert(\"RGB\")\n",
    "    image_rgb = np.array(image_rgb)\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        image_rgb, _f, top_labels=1, num_samples=100, segmentation_fn=segmentation_fn\n",
    "    )\n",
    "    _, explanation = explanation.get_image_and_mask(\n",
    "        explanation.top_labels[0],\n",
    "        positive_only=True,\n",
    "        num_features=len(explanation.segments),\n",
    "        hide_rest=False,\n",
    "    )\n",
    "    return explanation\n",
    "\n",
    "\n",
    "# PartitionExplainer explanation function\n",
    "def _partexp_explain(explainer, image):\n",
    "    image_t = transform(image)\n",
    "    image_npy = image_t.permute(1, 2, 0).numpy()\n",
    "    image_npy = np.expand_dims(image_npy, axis=0)\n",
    "\n",
    "    max_evals = 128\n",
    "    explanation = explainer(image_npy, max_evals=max_evals, fixed_context=0)\n",
    "    explanation = explanation.values[0].sum(axis=-1)\n",
    "    return explanation\n",
    "\n",
    "\n",
    "# h-Shap explanation function\n",
    "def _hshap_explain(explainer, image):\n",
    "    image_t = transform(image)\n",
    "    image_t = image_t.to(device)\n",
    "\n",
    "    min_s = 80\n",
    "    threshold_mode, threshold_value = \"relative\", 70\n",
    "    explanation = explainer.explain(\n",
    "        image,\n",
    "        label=1,\n",
    "        s=min_s,\n",
    "        threshold_mode=threshold_mode,\n",
    "        threshold_value=threshold_value,\n",
    "    )\n",
    "    return explanation\n",
    "\n",
    "\n",
    "explainers = [\n",
    "    {\"name\": \"Grad-CAM\", \"explainer\": _cam_init(), \"explain\": _cam_explain},\n",
    "    {\"name\": \"LIME\", \"explainer\": _lime_init(), \"explain\": _lime_init},\n",
    "    {\n",
    "        \"name\": \"PartitionExplainer\",\n",
    "        \"explainer\": _partexp_init(),\n",
    "        \"explain\": _partexp_explain,\n",
    "    },\n",
    "    {\"name\": \"h-Shap\", \"explainer\": _hshap_init(), \"explain\": _hshap_explain},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
