{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining the Predictions of a Convolutional Neural Network on Head CT Images\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sulam-Group/h-shap/blob/jaco/siim-ml-tutorial/demo/RSNA_ICH_detection/explain.ipynb)\n",
    "\n",
    "## Task\n",
    "\n",
    "Explain the predictions of a Convolutional Neural Network (CNN) trained to predict the presence of intracranial hemorrhage on the [RSNA 2019 Brain CT Hemorrhage Challenge dataset](https://www.kaggle.com/competitions/rsna-intracranial-hemorrhage-detection/data).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. Basic understanding of machine learning and deep learning.\n",
    "2. Programming in Python.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "1. Load a pre-trained classifier in PyTorch.\n",
    "2. Use the classifier to predict the presence of hemorrhage in test images.\n",
    "3. Explain the predictions of the classifier to detect intracranial hemorrhage.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This Jupyter Notebook was based on code by Jacopo Teneggi ([jtenegg1@jhu.edu](mailto:jtenegg1@jhu.edu))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pretrained model\n",
    "\n",
    "Here, we load a pretrained model which was trained on the [RSNA 2019 Brain CT Hemorrhage Challenge dataset](https://www.kaggle.com/competitions/rsna-intracranial-hemorrhage-detection/data). The model is trained on the binary classification problem of predicting `1` when an images contains any type of hemorrhage, or `0` when the image is healthy.\n",
    "\n",
    "This step requires PyTorch. Follow the instructions at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) to install PyTorch. Note that, although not required, this Jupyter Notebook supports execution on a GPU. When running on CPU, expect long runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as tf\n",
    "\n",
    "# Define the device to run on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained model\n",
    "torch.set_grad_enabled(False)\n",
    "model = torch.hub.load(\n",
    "    \"Sulam-Group/h-shap:jaco/siim-ml-tutorial\", \"rsnahemorrhagenet\", trust_repo=\"check\"\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Model to Predict on Images\n",
    "\n",
    "Here, we use the pretrained model to predict on 4 positive test images from the [CQ500 dataset](http://headctstudy.qure.ai/dataset). \n",
    "\n",
    "Ground-truth annotations of the bleeds are provided by the [BHX extension](https://physionet.org/content/bhx-brain-bounding-box/1.1/) dataset, and are highlighted with red solid lines in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import requests as req\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def download(url: str) -> io.BytesIO:\n",
    "    \"\"\"\n",
    "    A helper function to download an object from a url.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    url: str\n",
    "        The url to download the object from.\n",
    "    \"\"\"\n",
    "    res = req.get(url)\n",
    "    res.raise_for_status()\n",
    "    return io.BytesIO(res.content)\n",
    "\n",
    "\n",
    "def window(img: np.ndarray, WL: int, WW: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    A function that windows the values of an image at level\n",
    "    `WL` with width `WW` (in Hounsfield Units).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    img: np.ndarray\n",
    "        The image to window.\n",
    "    WL: int\n",
    "        The window level.\n",
    "    WW: int\n",
    "        The window width.\n",
    "    \"\"\"\n",
    "    image_min = WL - WW // 2\n",
    "    image_max = WL + WW // 2\n",
    "    img[img < image_min] = image_min\n",
    "    img[img > image_max] = image_max\n",
    "\n",
    "    img = (img - image_min) / (image_max - image_min)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def annotate(annotation_df: pd.DataFrame, ax: plt.Axes) -> None:\n",
    "    \"\"\"\n",
    "    A helper function to annotate an image with its ground-truth\n",
    "    annotations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    annotation_df: pd.DataFrame\n",
    "        The DataFrame containing the annotations.\n",
    "    ax: plt.Axes\n",
    "        The Axes containing the image to annotate.\n",
    "    \"\"\"\n",
    "    for _, annotation_row in annotation_df.iterrows():\n",
    "        annotation = annotation_row[\"data\"].replace(\"'\", '\"')\n",
    "        annotation = json.loads(annotation)\n",
    "        bbox_x = annotation[\"x\"]\n",
    "        bbox_y = annotation[\"y\"]\n",
    "        bbox_width = annotation[\"width\"]\n",
    "        bbox_height = annotation[\"height\"]\n",
    "\n",
    "        bbox = patches.Rectangle(\n",
    "            (bbox_x, bbox_y),\n",
    "            bbox_width,\n",
    "            bbox_height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(bbox)\n",
    "\n",
    "\n",
    "# Download images and ground truth annotations;\n",
    "# images are windowed with the standard brain\n",
    "# setting, i.e. WW = 80 and WL = 40 .\n",
    "base_url = \"https://github.com/Sulam-Group/h-shap/blob/jaco/siim-ml-tutorial/demo/RSNA_ICH_detection/images\"\n",
    "sop_ids = [\n",
    "    \"1.2.276.0.7230010.3.1.4.296485376.1.1521713007.1700822\",\n",
    "    \"1.2.276.0.7230010.3.1.4.296485376.1.1521713021.1704518\",\n",
    "    \"1.2.276.0.7230010.3.1.4.296485376.1.1521713469.1816851\",\n",
    "    \"1.2.276.0.7230010.3.1.4.296485376.1.1521713940.1946656\",\n",
    "]\n",
    "images = np.stack(\n",
    "    [\n",
    "        window(\n",
    "            np.load(download(f\"{base_url}/{sop_id}.npy?raw=true\")).astype(np.float32),\n",
    "            WL=40,\n",
    "            WW=80,\n",
    "        )\n",
    "        for sop_id in tqdm(sop_ids)\n",
    "    ]\n",
    ")\n",
    "gt = pd.read_csv(f\"{base_url}/gt.csv?raw=true\")\n",
    "\n",
    "# Visualize images and ground truth annotations,\n",
    "# predict the presence of hemorrhage\n",
    "# in each image using the pretrained model.\n",
    "_, axes = plt.subplots(1, len(images), figsize=(16, 9))\n",
    "for i in range(len(images)):\n",
    "    # Preprocess image for prediction\n",
    "    x = images[i]\n",
    "    x = tf.to_tensor(x)\n",
    "    x = x.unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "    x = tf.normalize(x, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # Use the model to predict the presence of hemorrhage\n",
    "    x = x.to(device)\n",
    "    output = model(x)\n",
    "    prediction = (output > 0.5).long()\n",
    "\n",
    "    # Visualize image and ground truth annotations\n",
    "    ax = axes[i]\n",
    "    ax.imshow(images[i], cmap=\"gray\")\n",
    "\n",
    "    annotation_df = gt[gt[\"SOPInstanceUID\"] == sop_ids[i]]\n",
    "    annotate(annotation_df, ax)\n",
    "\n",
    "    ax.set_title(f\"Prediction: {prediction.item()}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Model Predictions Using `h-Shap`\n",
    "\n",
    "[`h-shap`](https://github.com/Sulam-Group/h-shap) is a Python package that provides an exact, fast, and hierarchical implmentation of the [Shapley value](https://en.wikipedia.org/wiki/Shapley_value) to explain model predictions on images.\n",
    "\n",
    "`h-shap` produces **saliency map**---heatmaps where the intensity of a pixel represents its importance towards the model's prediction.\n",
    "\n",
    "For more information on `h-shap`, refer to the paper [_\"Fast Hierarchical Games for Image Explanations\"_](https://ieeexplore.ieee.org/document/9826424)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install h-shap\n",
    "!python -m pip install h-shap --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hshap import Explainer\n",
    "from skimage.filters import threshold_otsu\n",
    "from time import time\n",
    "\n",
    "# Load the reference value used\n",
    "# to mask features\n",
    "reference = torch.load(download(f\"{base_url}/reference.pt?raw=true\"))\n",
    "reference = reference.to(device)\n",
    "\n",
    "# Initialize the explainer and its parameters\n",
    "explainer = Explainer(model=model, background=reference)\n",
    "\n",
    "s = 64\n",
    "R = np.linspace(0, s, 4, endpoint=False)\n",
    "A = np.linspace(0, 2 * np.pi, 8, endpoint=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(16, 9))\n",
    "for i in range(len(images)):\n",
    "    # Preprocess image for explanation\n",
    "    x = images[i]\n",
    "    x = tf.to_tensor(x)\n",
    "    x = x.repeat(3, 1, 1)\n",
    "    x = tf.normalize(x, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    x = x.to(device)\n",
    "\n",
    "    # Use h-shap to explain the prediction of\n",
    "    # the model on the image\n",
    "    print(f\"Explaining image {i+1} ...\", end=\" \")\n",
    "    t0 = time()\n",
    "    explanation = explainer.cycle_explain(\n",
    "        x=x,\n",
    "        label=0,\n",
    "        s=s,\n",
    "        R=R,\n",
    "        A=A,\n",
    "        threshold_mode=\"absolute\",\n",
    "        threshold=0.0,\n",
    "        softmax_activation=False,\n",
    "        batch_size=2,\n",
    "        binary_map=False,\n",
    "    )\n",
    "    print(f\"done in {time() - t0:.2f} s\")\n",
    "\n",
    "    # Filter explanation using Otsu's\n",
    "    # method to remove noise\n",
    "    explanation = explanation.numpy()\n",
    "    _t = threshold_otsu(explanation.flatten())\n",
    "    explanation = explanation * (explanation > _t)\n",
    "    abs_values = np.abs(explanation.flatten())\n",
    "    _max = np.nanpercentile(abs_values, 99)\n",
    "\n",
    "    # Visualize image, annotation, and explanation\n",
    "    ax = axes[i]\n",
    "    ax.imshow(images[i], cmap=\"gray\")\n",
    "    ax.imshow(\n",
    "        explanation.squeeze(),\n",
    "        cmap=\"bwr\",\n",
    "        vmin=-_max,\n",
    "        vmax=_max,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "    annotation_df = gt[gt[\"SOPInstanceUID\"] == sop_ids[i]]\n",
    "    annotate(annotation_df, ax)\n",
    "\n",
    "    ax.set_title(\"Explanation\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cuda102')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd8c972ecb72e669b05b9af0bbaad01a2103da39053b5ad2ceb924e75319f022"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
